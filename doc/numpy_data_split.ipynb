{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaDsjoiCd5oF"
      },
      "source": [
        "### Importing packages and data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l7ZTLEdDd5oG",
        "outputId": "ff3e954c-33c4-4ef3-a0c2-f60558163409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ClimSim'...\n",
            "remote: Enumerating objects: 4506, done.\u001b[K\n",
            "remote: Counting objects: 100% (811/811), done.\u001b[K\n",
            "remote: Compressing objects: 100% (357/357), done.\u001b[K\n",
            "remote: Total 4506 (delta 461), reused 718 (delta 441), pack-reused 3695\u001b[K\n",
            "Receiving objects: 100% (4506/4506), 140.16 MiB | 29.50 MiB/s, done.\n",
            "Resolving deltas: 100% (1554/1554), done.\n",
            "Updating files: 100% (2068/2068), done.\n",
            "/content/ClimSim/ClimSim\n",
            "Processing /content/ClimSim/ClimSim\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (2023.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (2.14.0)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (1.6.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from climsim-utils==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->climsim-utils==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.10/dist-packages (from netCDF4->climsim-utils==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4->climsim-utils==0.0.1) (2023.7.22)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->climsim-utils==0.0.1) (2023.3.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->climsim-utils==0.0.1) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->climsim-utils==0.0.1) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->climsim-utils==0.0.1) (3.2.2)\n",
            "Building wheels for collected packages: climsim-utils\n",
            "  Building wheel for climsim-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for climsim-utils: filename=climsim_utils-0.0.1-py3-none-any.whl size=14489 sha256=c9bee0a7f2a4b1216959209039d3cf7a0fa8b37e0faf1ad6d31743f4e016b8c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nl0lsm0r/wheels/07/2a/cc/eb42675edeed67be576a3d1f3b55c8050f115a21cfe7e71e99\n",
            "Successfully built climsim-utils\n",
            "Installing collected packages: climsim-utils\n",
            "  Attempting uninstall: climsim-utils\n",
            "    Found existing installation: climsim-utils 0.0.1\n",
            "    Uninstalling climsim-utils-0.0.1:\n",
            "      Successfully uninstalled climsim-utils-0.0.1\n",
            "Successfully installed climsim-utils-0.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "climsim_utils"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/leap-stc/ClimSim.git\n",
        "%cd ClimSim\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF5XHiEPd5oH",
        "outputId": "bf16fd86-d6c3-4ce5-c8a6-6a8c90529d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import itertools\n",
        "import os\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koLaQplCp0nU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2-FiW1rSd5oH"
      },
      "outputs": [],
      "source": [
        "from climsim_utils.data_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX9vzQkid5oH"
      },
      "source": [
        "### Instantiating class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Nry8WjqZd5oH"
      },
      "outputs": [],
      "source": [
        "grid_path = '../ClimSim/grid_info/ClimSim_low-res_grid-info.nc'\n",
        "norm_path = './preprocessing/normalizations/'\n",
        "\n",
        "grid_info = xr.open_dataset(grid_path)\n",
        "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
        "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
        "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
        "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
        "\n",
        "data = data_utils(grid_info = grid_info,\n",
        "                  input_mean = input_mean,\n",
        "                  input_max = input_max,\n",
        "                  input_min = input_min,\n",
        "                  output_scale = output_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-yFBT6oZD3U1"
      },
      "outputs": [],
      "source": [
        "data.data_path='../ClimSim/e3smdata'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsOFxk1f2B9F",
        "outputId": "6c5960e1-7b36-4ff6-c9f8-fbe43fc97faf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['E3SM-MMF.mli.0002-01-02-00000.nc',\n",
              " 'E3SM-MMF.mli.0002-01-02-01200.nc',\n",
              " 'E3SM-MMF.mli.0002-01-02-02400.nc',\n",
              " 'E3SM-MMF.mli.0002-01-02-03600.nc',\n",
              " 'E3SM-MMF.mli.0002-01-02-04800.nc']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def expand_pattern(pattern):\n",
        "    \"\"\"generate every possible values\"\"\"\n",
        "    matches = re.findall(r'\\[([^\\]]+)\\]', pattern)\n",
        "    replacements = [list(match) for match in matches]\n",
        "\n",
        "    for replacement in itertools.product(*replacements):\n",
        "        temp_pattern = pattern\n",
        "        for r in replacement:\n",
        "            temp_pattern = re.sub(r'\\[([^\\]]+)\\]', r, temp_pattern, 1)\n",
        "        yield temp_pattern\n",
        "\n",
        "def generate_filenames_with_times(pattern, end_time=85200):\n",
        "    date_pattern, _ = pattern.split('-*')\n",
        "\n",
        "    expanded_patterns = expand_pattern(date_pattern)\n",
        "\n",
        "    times = range(0, end_time + 1, 1200)\n",
        "\n",
        "\n",
        "    filenames = []\n",
        "    for date in expanded_patterns:\n",
        "        for time in times:\n",
        "            filename = f\"E3SM-MMF.mli.{date}-{time:05}.nc\"\n",
        "            filenames.append(filename)\n",
        "\n",
        "    return filenames\n",
        "\n",
        "# change the pattern\n",
        "pattern = \"000[2]-0[1]-0[2]-*\"\n",
        "file_names = generate_filenames_with_times(pattern, 85200)\n",
        "\n",
        "\n",
        "file_names[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fiSTmr6hh3z",
        "outputId": "182936cc-729f-4d81-b4cb-c3cf4d8f8522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "# Directory to save files\n",
        "save_dir = \"../ClimSim/e3smdata\"\n",
        "\n",
        "# Ensure save directory exists\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "# Download loop\n",
        "base_url = \"https://huggingface.co/datasets/LEAP/ClimSim_low-res/resolve/main/\"\n",
        "\n",
        "\n",
        "for file_name in file_names[:10]:\n",
        "\n",
        "\n",
        "    match = re.search(r'\\.(\\d{4})-(\\d{2})-', file_name)\n",
        "    if match:\n",
        "        year, month = match.groups()\n",
        "        url = f\"{base_url}train/{year}-{month}/{file_name}\"\n",
        "    else:\n",
        "        print(f\"Cannot determine year and month for {file_name}\")\n",
        "        continue\n",
        "\n",
        "    # downloading\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(os.path.join(save_dir, file_name), 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        print(f\"Failed to download {file_name}\")\n",
        "\n",
        "print(\"Download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UyldxyQf7xvB",
        "outputId": "47111399-cf7d-44c0-bf19-430bfc7caea9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/datasets/LEAP/ClimSim_low-res/resolve/main/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4O49Y-hRD3A4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# directory_path = '../ClimSim/e3smdata'\n",
        "\n",
        "\n",
        "# for filename in os.listdir(directory_path):\n",
        "#     # connect path\n",
        "#     file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "#     # check files\n",
        "#     if os.path.isfile(file_path):\n",
        "#         print(filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWcQPElnd5oH"
      },
      "source": [
        "### Create training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os; os.makedirs(\"sampledata\", exist_ok=True)\n",
        "# os.makedirs(\"e3smdata\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "V1kxP0T52eL9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2mu7DM9dd5oI",
        "outputId": "5f4ecbfe-ad76-4457-c1ce-345977126dd5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a225ec60bcb0>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# save numpy files of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_as_npy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../ClimSim/sampledata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ClimSim/climsim_utils/data_utils.py\u001b[0m in \u001b[0;36msave_as_npy\u001b[0;34m(self, data_split, save_path, save_latlontime_dict)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_ncdata_with_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mnpy_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mnpy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnpy_iterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mnpy_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnpy_iterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4689\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4693\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    773\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3028\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3029\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3030\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5887\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5888\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} FileNotFoundError: [Errno 2] No such file or directory: '/content/ClimSim/ClimSim/e3smdata/E3SM-MMF.mlo.0002-01-02-00000.nc'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\", line 211, in _acquire_with_cache_info\n    file = self._cache[self._key]\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/lru_cache.py\", line 56, in __getitem__\n    value = self._cache[key]\n\nKeyError: [<class 'netCDF4._netCDF4.Dataset'>, ('/content/ClimSim/ClimSim/e3smdata/E3SM-MMF.mlo.0002-01-02-00000.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '79cfda02-2483-48a9-bc13-440c273d139f']\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/content/ClimSim/climsim_utils/data_utils.py\", line 356, in gen\n    ds_target = self.get_target(file)\n\n  File \"/content/ClimSim/climsim_utils/data_utils.py\", line 257, in get_target\n    ds_target = self.get_xrdata(input_file.replace('.mli.','.mlo.'))\n\n  File \"/content/ClimSim/climsim_utils/data_utils.py\", line 236, in get_xrdata\n    ds = xr.open_dataset(file, engine = 'netcdf4')\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/api.py\", line 570, in open_dataset\n    backend_ds = backend.open_dataset(\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/netCDF4_.py\", line 602, in open_dataset\n    store = NetCDF4DataStore.open(\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/netCDF4_.py\", line 400, in open\n    return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/netCDF4_.py\", line 347, in __init__\n    self.format = self.ds.data_model\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/netCDF4_.py\", line 409, in ds\n    return self._acquire()\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/netCDF4_.py\", line 403, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n\n  File \"/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n\n  File \"src/netCDF4/_netCDF4.pyx\", line 2469, in netCDF4._netCDF4.Dataset.__init__\n\n  File \"src/netCDF4/_netCDF4.pyx\", line 2028, in netCDF4._netCDF4._ensure_nc_success\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/ClimSim/ClimSim/e3smdata/E3SM-MMF.mlo.0002-01-02-00000.nc'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ],
      "source": [
        "# set inputs and outputs to V1 subset\n",
        "data.set_to_v1_vars()\n",
        "\n",
        "# set regular expressions for selecting training data\n",
        "data.set_regexps(data_split = 'train',\n",
        "                 regexps = ['E3SM-MMF.mli.000[1234567]-*-*-*.nc', # years 1 through 7\n",
        "                            'E3SM-MMF.mli.0008-01-*-*.nc']) # first month of year 8\n",
        "# set temporal subsampling\n",
        "data.set_stride_sample(data_split = 'train', stride_sample = 7)\n",
        "# create list of files to extract data from\n",
        "\n",
        "data.set_filelist(data_split = 'train')\n",
        "\n",
        "# first_file = data.get_filelist('train')[0]\n",
        "# ds = data.get_xrdata(first_file)\n",
        "# print(list(ds.variables))\n",
        "\n",
        "# save numpy files of training data\n",
        "data.save_as_npy(data_split = 'train', save_path = '../ClimSim/sampledata')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4bPj-42Iftb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0pLVJad5oI"
      },
      "source": [
        "### Create validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XPdLWRid5oI"
      },
      "outputs": [],
      "source": [
        "# set regular expressions for selecting validation data\n",
        "data.set_regexps(data_split = 'val',\n",
        "                 regexps = ['E3SM-MMF.mli.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
        "                            'E3SM-MMF.mli.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
        "                            'E3SM-MMF.mli.0009-01-*-*.nc']) # first month of year 9\n",
        "# set temporal subsampling\n",
        "data.set_stride_sample(data_split = 'val', stride_sample = 7)\n",
        "# create list of files to extract data from\n",
        "data.set_filelist(data_split = 'val')\n",
        "# save numpy files of validation data\n",
        "data.save_as_npy(data_split = 'val', save_path = '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyPJ9gMVd5oI"
      },
      "source": [
        "### Create scoring data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJMkmH3-d5oI"
      },
      "outputs": [],
      "source": [
        "# set regular expressions for selecting scoring data (stride of 6 is needed for daily averaging)\n",
        "data.set_regexps(data_split = 'scoring',\n",
        "                 regexps = ['E3SM-MMF.mli.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
        "                            'E3SM-MMF.mli.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
        "                            'E3SM-MMF.mli.0009-01-*-*.nc']) # first month of year 9\n",
        "# set temporal subsampling\n",
        "data.set_stride_sample(data_split = 'scoring', stride_sample = 6)\n",
        "# create list of files to extract data from\n",
        "data.set_filelist(data_split = 'scoring')\n",
        "# save numpy files of scoring data\n",
        "data.save_as_npy(data_split = 'scoring', save_path = '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e-34rRed5oI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbRPjdRyAkSg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22aoa80ygSTg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZYnNa5BtgSVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42bbbaf-4c27-4cb8-f413-39c502925d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.18.0 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLPbKN87gSYB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inS8n6J_QVGf"
      },
      "outputs": [],
      "source": [
        "pip install netcdf4 h5netcdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVwM0lEbQ4Xr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzAv2uVLAkUq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3LnCSLHRamk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkHp8jcXAkWz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc7QhozMNGZO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_c9abRAkYx"
      },
      "outputs": [],
      "source": [
        "pip install netcdf4 h5netcdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OUeM9op_zqa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgIZn7RyAdVg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeB2x3jbEDGJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDLMmAR-k-5i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4LOCR_DGJl2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGh5v5wUI69v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNyxfWRbS3rr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}